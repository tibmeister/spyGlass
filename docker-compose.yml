version: "3.3"

secrets:
  GF_SMTP_PASS:
    external: true
  GF_SMTP_HOST:
    external: true
  GF_SMTP_USER:
    external: true

networks:
  net:
    driver: overlay
    attachable: true

volumes:
    alertmanager: {}

configs:
  node_rules:
    file: /nfs/swarm/dockerfiles/spyGlass/spyGlass-prometheus/rules/swarm_node.rules.yml
  task_rules:
    file: /nfs/swarm/dockerfiles/spyGlass/spyGlass-prometheus/rules/swarm_task.rules.yml

services:
  cadvisor:
    image: google/cadvisor
    networks:
      - net
    command: -logtostderr -docker_only
    ports:
      - 8780:8080
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /:/rootfs:ro
      - /var/run:/var/run
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    depends_on:
      - redis
    deploy:
      mode: global
      placement:
        constraints:
          - node.role != manager
      resources:
        limits:
          memory: 128M

  redis:
    image: redis:latest
    container_name: redis
    ports:
      - 6379:6379

  renderer:
    image: grafana/grafana-image-renderer:latest
    environment:
      - ENABLE_METRICS=true

  grafana:
    image: tibmeister/spyglass_grafana
    networks:
      - net
    secrets:
      - GF_SMTP_PASS
    environment:
      #- GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}
      #- GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=natel-usgs-datasource,grafana-piechart-panel,briangann-gauge-panel,michaeldmoore-multistat-panel,grafana-clock-panel,flant-statusmap-panel,flant-statusmap-panel,larona-epict-panel,simpod-json-datasource,farski-blendstat-panel,blackmirror1-statusbygroup-panel,mxswat-separator-panel,grafana-worldmap-panel
      #- GF_SERVER_PROTOCOL=https
      #- GF_SERVER_CERT_FILE__FILE=/tmp/certs/fullchain.cer
      #- GF_SERVER_CERT_KEY__FILE=/tmp/certs/cheron.tiberiansun.us.key
      #- GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL:-localhost}
      - GF_SMTP_ENABLED=true
      - GF_SMTP_FROM_ADDRESS__FILE=/var/run/secrets/GF_SMTP_USER
      - GF_SMTP_FROM_NAME=Grafana
      - GF_SMTP_HOST__FILE=/var/run/secrets/GF_SMTP_HOST
      - GF_SMTP_USER__FILE=/var/run/secrets/GF_SMTP_USER
      - GF_SMTP_PASSWORD__FILE=/var/run/secrets/GF_SMTP_PASS
      - GF_RENDERING_SERVER_URL=http://renderer:8081
      - GF_RENDERING_CALLBACK_URL=http://grafana:3000/
      - GF_LOG_FILTERS=rendering:debug 
    volumes:
      - /nfs/swarm/spyGlassData/grafana:/var/lib/grafana
      #- "/nfs/swarm/LetsEncrypt/cheron.tiberiansun.us:/tmp/certs/:ro"
    ports:
      - 3000:3000
      #- 8123:8123
    secrets:
      - GF_SMTP_PASS
      - GF_SMTP_USER
      - GF_SMTP_HOST
    logging:
      driver: json-file
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role != manager
      resources:
        limits:
          memory: 512M

#  alertmanager:
#    image: tibmeister/spyglass_am
#    networks:
#      - net
#    environment:
#      - SLACK_URL=${SLACK_URL:-https://hooks.slack.com/services/TOKEN}
#      - SLACK_CHANNEL=${SLACK_CHANNEL:-general}
#      - SLACK_USER=${SLACK_USER:-alertmanager}
#    command:
#      - '--config.file=/etc/alertmanager/alertmanager.yml'
#      - '--storage.path=/alertmanager'
#    volumes:
#      - alertmanager:/alertmanager
#    ports:
#      - 9093:9093
#    deploy:
#      mode: replicated
#      replicas: 1
#      placement:
#        constraints:
#          - node.role != manager
#      resources:
#        limits:
#          memory: 12M
#        reservations:
#          memory: 8M

#  unsee:
#    image: cloudflare/unsee:v0.8.0
#    networks:
#      - net
#    environment:
#      - "ALERTMANAGER_URIS=default:http://alertmanager:9093"
#    ports:
#      - 9080:8080
#    deploy:
#      mode: replicated
#      replicas: 1
#      placement:
#        constraints:
#          - node.role != manager
#      resources:
#        limits:
#          memory: 12M
#        reservations:
#          memory: 8M

  node-exporter:
    image: tibmeister/spyglass-node-exporter
    networks:
      - net
    environment:
      - NODE_ID={{.Node.ID}}
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /etc/hostname:/etc/nodename
    command:
      - '--path.sysfs=/host/sys'
      - '--path.procfs=/host/proc'
      - '--collector.textfile.directory=/etc/node-exporter/'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
      # no collectors are explicitely enabled here, because the defaults are just fine,
      # see https://github.com/prometheus/node_exporter
      # disable ipvs collector because it barfs the node-exporter logs full with errors on my centos 7 vm's
      - '--no-collector.ipvs'
    deploy:
      mode: global
      placement:
        constraints:
          - node.role != manager
      resources:
        limits:
          memory: 12M

  graphite:
    image: graphiteapp/graphite-statsd:1.1.7-6
    networks:
      - net
    ports:
      - 80:80
      - 2003:2003
      - 2004:2004
      - 2023:2023
      - 2024:2024
      - 8125:8125/udp
      - 8126:8126
    volumes:
      - type: bind
        source: /nfs/swarm/spyGlassData/graphite/conf
        target: /opt/graphite/conf
      - type: bind
        source: /nfs/swarm/spyGlassData/graphite/storage
        target: /opt/graphite/storage
      - type: bind
        source: /nfs/swarm/spyGlassData/statsd/config
        target: /opt/statsd/config
    deploy:
      mode: replicated
      replicas: 1 
      placement:
        constraints:
          - node.role != manager
      resources:
        limits:
          memory: 512M

  influx-General:
    image: influxdb:1.7.7
    networks:
      - net
    environment:
      - INFLUXDB_DB=influx-systems
      - INFLUXDB_HTTP_AUTH_ENABLED=false
      - INFLUXDB_UDP_ENABLED=true
      #- INFLUXDB_RETENTION_ENABLED=true
      #- INFLUXDB_RETENTION_CHECK_INTERVAL=1440m0s
    volumes:
      - /nfs/swarm/influxDB/General:/var/lib/influxdb
    ports:
      - 8186:8086
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role != manager
      resources:
        limits:
          memory: 1024M
      
  influx-pfSense:
    image: influxdb:1.7.7
    networks:
      - net
    environment:
      - INFLUXDB_DB=pfSense
      - INFLUXDB_HTTP_AUTH_ENABLED=false
    volumes:
      - /nfs/swarm/influxDB/pfSense:/var/lib/influxdb
    ports:
      - 8084:8086
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role != manager
      resources:
        limits:
          memory: 1024M

  influxSNMP:
    image: influxdb:1.7.7
    networks:
      - net
    environment:
      - INFLUXDB_DB=grafana
      - INFLUXDB_HTTP_AUTH_ENABLED=false
    volumes:
      - /nfs/swarm/influxDB/SNMP:/var/lib/influxdb
    ports:
      - 8086:8086
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role != manager
      resources:
        limits:
          memory: 1024M

  telegrafSNMP:
    image: nuntz/telegraf-snmp
    depends_on:
      - influxSNMP
    networks:
      - net
    volumes:
      - type: bind
        source: /nfs/swarm/dockerfiles/spyGlass/spyGlass-telegrafSNMP/mibs
        target: /root/.snmp/mibs
      - type: bind
        source: /nfs/swarm/dockerfiles/spyGlass/spyGlass-telegrafSNMP/telegraf
        target: /etc/telegraf
    deploy:
      resources:
        limits:
          memory: 128M

  telegraf-OWM:
    image: telegraf:1.14
    depends_on:
      - influx-General
    networks:
      - net
    volumes:
      - type: bind
        source: /nfs/swarm/dockerfiles/spyGlass/spyGlass-telegrafOWM/telegraf
        target: /etc/telegraf
    deploy:
      resources:
        limits:
          memory: 128M

  prometheus:
    image: tibmeister/spyglass-prometheus
    networks:
      - net
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention=24h'
    volumes:
      - /nfs/swarm/prometheus:/prometheus
    configs:
      - source: node_rules
        target: /etc/prometheus/swarm_node.rules.yml
      - source: task_rules
        target: /etc/prometheus/swarm_task.rules.yml
    ports:
      - 9090:9090
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role != manager
      resources:
        limits:
          memory: 1024M
